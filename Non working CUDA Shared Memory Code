#include <cuda.h>
#include <curand_kernel.h>
#include <iostream>
#include <vector>
#include <cmath>
#include <ctime>

#define L 100
#define N (L*L)
#define J 1.00
#define IT 5e8 // Number of iterations, should be divisible by 2 for even updates
#define NTHREADS 256 // Number of GPU threads
#define BLOCK_DIMX 16 // Adjust this to your desired block dimension X
#define BLOCK_DIMY 16 // Adjust this to your desired block dimension Y

__device__ int get_index(int row, int col) {
    return (row * L + col) % N;
}

// The lattice uses boolean values, true for spin up (equivalent to 1) and false for spin down (equivalent to -1)
__device__ int delta_energy(bool* lattice, int r, int c) {
    int sum = lattice[get_index((r - 1 + L) % L, c)]
        + lattice[get_index((r + 1) % L, c)]
        + lattice[get_index(r, (c - 1 + L) % L)]
        + lattice[get_index(r, (c + 1) % L)];
    sum = 2 * sum - 4; // Convert sum from [0, 4] to [-4, 4] to match the original spin values
    int spin = lattice[get_index(r, c)] ? 1 : -1; // Convert bool to equivalent spin value
    return 2 * spin * sum;
}

__global__ void flip_spins_with_shared(bool* lattice, float* prob, float* energy, int* M, curandState* states, bool update_black) {
    extern __shared__ bool shared_lattice[];

    int globalIdxX = blockIdx.x * blockDim.x + threadIdx.x;
    int globalIdxY = blockIdx.y * blockDim.y + threadIdx.y;
    int threadId = threadIdx.y * blockDim.x + threadIdx.x;
    int localIdxX = threadIdx.x + 1;
    int localIdxY = threadIdx.y + 1;

    // Loading lattice data to shared memory including halo cells for neighbors
    shared_lattice[threadId] = lattice[get_index(globalIdxY, globalIdxX)];

    // Loading halo cells into shared memory
    if (threadIdx.x == 0)
        shared_lattice[threadId - 1] = lattice[get_index(globalIdxY, globalIdxX - 1)];
    if (threadIdx.x == blockDim.x - 1)
        shared_lattice[threadId + 1] = lattice[get_index(globalIdxY, globalIdxX + 1)];
    if (threadIdx.y == 0)
        shared_lattice[threadId - blockDim.x] = lattice[get_index(globalIdxY - 1, globalIdxX)];
    if (threadIdx.y == blockDim.y - 1)
        shared_lattice[threadId + blockDim.x] = lattice[get_index(globalIdxY + 1, globalIdxX)];

    __syncthreads();

    if (globalIdxX >= L || globalIdxY >= L) return;

    bool is_black = ((globalIdxY + globalIdxX) % 2 == 0);

    if (is_black == update_black) {
        int localIdx = localIdxY * (BLOCK_DIMX + 2) + localIdxX;

        // Compute the delta energy using the shared lattice
        int sum = shared_lattice[localIdx - 1 - (BLOCK_DIMX + 2)]
            + shared_lattice[localIdx + 1 - (BLOCK_DIMX + 2)]
            + shared_lattice[localIdx - (BLOCK_DIMX + 2) - 1]
            + shared_lattice[localIdx - (BLOCK_DIMX + 2) + 1];
        sum = 2 * sum - 4;
        int spin = shared_lattice[localIdx] ? 1 : -1;
        int delta = 2 * spin * sum;

        float rnd = curand_uniform(&states[globalIdxY * L + globalIdxX]);

        if (delta <= 0 || (delta == 4 && rnd < prob[0]) || (delta == 8 && rnd < prob[1])) {
            shared_lattice[localIdx] = !shared_lattice[localIdx]; // Update shared memory
            lattice[get_index(globalIdxY, globalIdxX)] = !lattice[get_index(globalIdxY, globalIdxX)]; // Update global memory
            atomicAdd(energy, delta * J);
            int spin_change = shared_lattice[localIdx] ? 2 : -2;
            atomicAdd(M, spin_change);
        }
    }
}


__global__ void setup_rand_kernel(curandState* state, unsigned long seed) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        curand_init(seed, idx, 0, &state[idx]);
    }
}

__global__ void initialize_lattice_kernel(bool* lattice, float* energy, int* M, curandState* states) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        float randVal = curand_uniform(&states[idx]);
        lattice[idx] = (randVal < 0.5f) ? true : false;

        // Calculate magnetization
        atomicAdd(M, lattice[idx] ? 1 : -1);
    }
}
int main() {
    bool* dev_lattice;
    cudaMalloc((void**)&dev_lattice, N * sizeof(bool));

    curandState* dev_states;
    cudaMalloc((void**)&dev_states, N * sizeof(curandState));

    dim3 blocksPerGrid((N + NTHREADS - 1) / NTHREADS, 1, 1);
    dim3 threadsPerBlock(NTHREADS, 1, 1);

    unsigned long seed = static_cast<unsigned long>(time(nullptr));
    setup_rand_kernel << <blocksPerGrid, threadsPerBlock >> > (dev_states, seed);

    float* dev_energy;
    int* dev_M;
    cudaMalloc((void**)&dev_energy, sizeof(float));
    cudaMalloc((void**)&dev_M, sizeof(int));

    float energy = 0.0f;
    int M = 0;
    cudaMemcpy(dev_energy, &energy, sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(dev_M, &M, sizeof(int), cudaMemcpyHostToDevice);

    initialize_lattice_kernel << <blocksPerGrid, threadsPerBlock >> > (dev_lattice, dev_energy, dev_M, dev_states);

    float* dev_probabilities;
    cudaMalloc((void**)&dev_probabilities, 2 * sizeof(float));

    for (float T = 0.2f; T <= 3.0f; T += 0.1f) {
        clock_t start_time = clock();

        float prob[2] = { exp(-4 * J / T), exp(-8 * J / T) };
        cudaMemcpy(dev_probabilities, prob, 2 * sizeof(float), cudaMemcpyHostToDevice);

        dim3 blockSize(BLOCK_DIMX, BLOCK_DIMY);
        dim3 gridSize((L + blockSize.x - 1) / blockSize.x, (L + blockSize.y - 1) / blockSize.y);
        int sharedMemSize = (BLOCK_DIMX + 2) * (BLOCK_DIMY + 2) * sizeof(bool); // Padding for halo

        // Ensure an even number of iterations for a complete Monte Carlo sweep.
        for (unsigned long i = 0; i < IT / N; i += 2) {
            flip_spins_with_shared << <gridSize, blockSize, sharedMemSize >> > (dev_lattice, dev_probabilities, dev_energy, dev_M, dev_states, true);
            flip_spins_with_shared << <gridSize, blockSize, sharedMemSize >> > (dev_lattice, dev_probabilities, dev_energy, dev_M, dev_states, false);
        }

        cudaDeviceSynchronize();

        clock_t end_time = clock();

        double elapsed_secs = static_cast<double>(end_time - start_time) / CLOCKS_PER_SEC;

        cudaMemcpy(&energy, dev_energy, sizeof(float), cudaMemcpyDeviceToHost);
        cudaMemcpy(&M, dev_M, sizeof(int), cudaMemcpyDeviceToHost);

        std::cout << "Temperature: " << T << std::endl;
        std::cout << "Final Energy: " << energy / N << std::endl;
        std::cout << "Final Magnetization: " << static_cast<float>(M) / N << std::endl;
        std::cout << "Simulation time (seconds): " << elapsed_secs << std::endl << std::endl;
    }

    cudaFree(dev_lattice);
    cudaFree(dev_states);
    cudaFree(dev_energy);
    cudaFree(dev_M);
    cudaFree(dev_probabilities);

    return 0;
}
